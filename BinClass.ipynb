{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logan Tucker\n",
    "Task 1 : Binary Classification\n",
    "CS74 \n",
    "S23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Phase\n",
    "    step 1) load in data \n",
    "    step 2) classify reviews for each cutoff\n",
    "    step 3) clean data\n",
    "    step 4) vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "      <th>style</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>11 12, 2016</td>\n",
       "      <td>C413C78E10E54C5DB41837889F36C1E8</td>\n",
       "      <td>565D194F38B1CC3F806EE677C61F639C</td>\n",
       "      <td>465E154EC79AFFAB5EB2607198B21433</td>\n",
       "      <td>all of the reviews for this product are fake.</td>\n",
       "      <td>All fake reviews, beware.</td>\n",
       "      <td>1478908800</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Size:': ' Polaris H4'}</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12 6, 2016</td>\n",
       "      <td>490AE37808EFEE3AF4FE6DEBDEB5A4C8</td>\n",
       "      <td>0D66512A0A7F580523AB996378DF0F14</td>\n",
       "      <td>760C63E8E5E8DC3FAA01878D37BA5678</td>\n",
       "      <td>wrong part. our fault.</td>\n",
       "      <td>One Star</td>\n",
       "      <td>1480982400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>09 17, 2014</td>\n",
       "      <td>74A9FA5A64449BEE2A2E8E3F62872F0F</td>\n",
       "      <td>A0E45600FF2C5A779CB4314F379C253A</td>\n",
       "      <td>C6E4DD5C1C4EC09E90182644ED6CA9EF</td>\n",
       "      <td>this wire set it really sucks!!!</td>\n",
       "      <td>One Star</td>\n",
       "      <td>1410912000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>06 11, 2016</td>\n",
       "      <td>EB561158A2829D98B467FE03CC1E45F1</td>\n",
       "      <td>37AB9A82470595E0ACB88BAC48C150EE</td>\n",
       "      <td>F4892A77EA45C52F40AB17ED537EF9FF</td>\n",
       "      <td>first use, it leaked instantly. even at 5 buck...</td>\n",
       "      <td>One Star</td>\n",
       "      <td>1465603200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Color:': ' Clear', 'Style:': ' 45 Degree'}</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12 23, 2017</td>\n",
       "      <td>5045D801332850D21618DD13A697CD9B</td>\n",
       "      <td>5772FF30428EEB8E0258C1A53CA2EC50</td>\n",
       "      <td>522F0BBFF2B47F1D63FF781A0AB1D079</td>\n",
       "      <td>didn't fit</td>\n",
       "      <td>One Star</td>\n",
       "      <td>1513987200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime                        reviewerID  \\\n",
       "0        1     False  11 12, 2016  C413C78E10E54C5DB41837889F36C1E8   \n",
       "1        1      True   12 6, 2016  490AE37808EFEE3AF4FE6DEBDEB5A4C8   \n",
       "2        1      True  09 17, 2014  74A9FA5A64449BEE2A2E8E3F62872F0F   \n",
       "3        1      True  06 11, 2016  EB561158A2829D98B467FE03CC1E45F1   \n",
       "4        1      True  12 23, 2017  5045D801332850D21618DD13A697CD9B   \n",
       "\n",
       "                               asin                      reviewerName  \\\n",
       "0  565D194F38B1CC3F806EE677C61F639C  465E154EC79AFFAB5EB2607198B21433   \n",
       "1  0D66512A0A7F580523AB996378DF0F14  760C63E8E5E8DC3FAA01878D37BA5678   \n",
       "2  A0E45600FF2C5A779CB4314F379C253A  C6E4DD5C1C4EC09E90182644ED6CA9EF   \n",
       "3  37AB9A82470595E0ACB88BAC48C150EE  F4892A77EA45C52F40AB17ED537EF9FF   \n",
       "4  5772FF30428EEB8E0258C1A53CA2EC50  522F0BBFF2B47F1D63FF781A0AB1D079   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0      all of the reviews for this product are fake.   \n",
       "1                             wrong part. our fault.   \n",
       "2                   this wire set it really sucks!!!   \n",
       "3  first use, it leaked instantly. even at 5 buck...   \n",
       "4                                         didn't fit   \n",
       "\n",
       "                     summary  unixReviewTime  vote image  \\\n",
       "0  All fake reviews, beware.      1478908800   2.0   NaN   \n",
       "1                   One Star      1480982400   NaN   NaN   \n",
       "2                   One Star      1410912000   NaN   NaN   \n",
       "3                   One Star      1465603200   NaN   NaN   \n",
       "4                   One Star      1513987200   NaN   NaN   \n",
       "\n",
       "                                          style    category  \n",
       "0                      {'Size:': ' Polaris H4'}  automotive  \n",
       "1                                           NaN  automotive  \n",
       "2                                           NaN  automotive  \n",
       "3  {'Color:': ' Clear', 'Style:': ' 45 Degree'}  automotive  \n",
       "4                                           NaN  automotive  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# upload Training.csv and Test.csv \n",
    "training = pd.read_csv('Training.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify Reviews\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(rating, cutoff):\n",
    "    if rating <= cutoff:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "for cutoff in [1, 2, 3, 4]:\n",
    "    training['label ' + str(cutoff)] = training['overall'].apply(lambda x: label(x, cutoff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for blank spots\n",
    "training.fillna('', inplace = True)\n",
    "test.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize data\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# set up train test split\n",
    "x1 = training['reviewText']\n",
    "x2 = training[\"summary\"]\n",
    "y = training['label 4']\n",
    "\n",
    "# vectorize the text \n",
    "vectorizer1 = TfidfVectorizer(ngram_range = (1, 2))\n",
    "vectorizer2 = TfidfVectorizer(ngram_range = (1, 2))\n",
    "\n",
    "# get review text frequency dict\n",
    "x1 = vectorizer1.fit_transform(x1)\n",
    "x2 = vectorizer2.fit_transform(x2)\n",
    "\n",
    "x = hstack((x1, x2))\n",
    "\n",
    "# create train test split for cross\n",
    "train_x_tf, test_x_tf, train_y, test_y = train_test_split(x, y, test_size=.1, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier #1 : Naive bayes\n",
    "    step 1) train\n",
    "    step 2) report\n",
    "    step 3) tune\n",
    "    step 4) report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "\n",
      "0.8574854402192532\n"
     ]
    }
   ],
   "source": [
    "# perform naive bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_x_tf, train_y)\n",
    "\n",
    "# predict the label of the test samples here using the model\n",
    "test_predicted_nb = nb.predict(test_x_tf)\n",
    "\n",
    "# get model acuracy score\n",
    "print(\"Accuracy: \\n\")\n",
    "print(accuracy_score(test_y, test_predicted_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE - tune report\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "[[2360    2]\n",
      " [ 414  143]]\n",
      "\n",
      "Macro F1-score: 0.66\n",
      "\n",
      "ROC AUC: 0.6279428777304326\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      2774\n",
      "           1       0.26      0.99      0.41       145\n",
      "\n",
      "    accuracy                           0.86      2919\n",
      "   macro avg       0.63      0.92      0.66      2919\n",
      "weighted avg       0.96      0.86      0.89      2919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_y, test_predicted_nb)\n",
    "print('Confusion Matrix:\\n')\n",
    "print(confusion)\n",
    "\n",
    "# f1\n",
    "print('\\nMacro F1-score: {:.2f}\\n'.format(f1_score(test_y, test_predicted_nb, average='macro')))\n",
    "\n",
    "# roc auc score\n",
    "fpr, tpr, t =  roc_curve(test_y, test_predicted_nb)\n",
    "roc_score = auc(fpr, tpr)\n",
    "print(\"ROC AUC:\", roc_score)\n",
    "\n",
    "# classification report\n",
    "print(\"\\nClassification report: \\n\")\n",
    "print(classification_report(test_predicted_nb, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertune\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: \n",
      "{'alpha': 0.1, 'fit_prior': False, 'force_alpha': True}\n",
      "\n",
      "Best accuracy: \n",
      "0.8693566806242862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_nb = [{\n",
    "    \"alpha\": [0.1, 0.5, 1.0, 1.5, 2.0], \n",
    "    \"force_alpha\": [True, False], \n",
    "    \"fit_prior\": [True, False]\n",
    "}]\n",
    "\n",
    "nb_gs = MultinomialNB()\n",
    "\n",
    "grid_search = GridSearchCV(nb_gs, param_grid_nb, cv=5, scoring=\"accuracy\", return_train_score=True)\n",
    "\n",
    "# train it\n",
    "grid_search.fit(train_x_tf, train_y)\n",
    "\n",
    "# get model with best params\n",
    "best_params_nb = grid_search.best_params_\n",
    "print(\"Best parameters: \")\n",
    "print(best_params_nb)\n",
    "\n",
    "# get best score\n",
    "best_score_nb = grid_search.best_score_\n",
    "print(\"\\nBest accuracy: \")\n",
    "print(best_score_nb)\n",
    "\n",
    "# get predected y of best model\n",
    "tuned_test_predicted_nb = grid_search.predict(test_x_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST - tune report\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      "[[2136  226]\n",
      " [ 187  370]]\n",
      "\n",
      "Macro F1-score: 0.78\n",
      "\n",
      "ROC AUC: 0.784295632371921\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91      2323\n",
      "           1       0.66      0.62      0.64       596\n",
      "\n",
      "    accuracy                           0.86      2919\n",
      "   macro avg       0.78      0.77      0.78      2919\n",
      "weighted avg       0.86      0.86      0.86      2919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_y, tuned_test_predicted_nb)\n",
    "print('Confusion Matrix: \\n')\n",
    "print(confusion)\n",
    "\n",
    "# f1\n",
    "print('\\nMacro F1-score: {:.2f}\\n'.format(f1_score(test_y, tuned_test_predicted_nb, average='macro')))\n",
    "\n",
    "# roc auc score\n",
    "fpr, tpr, t =  roc_curve(test_y, tuned_test_predicted_nb)\n",
    "roc_score = auc(fpr, tpr)\n",
    "print(\"ROC AUC:\", roc_score)\n",
    "\n",
    "# classification report\n",
    "print(\"\\nClassification report: \\n\")\n",
    "print(classification_report(tuned_test_predicted_nb, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier #2 : Logistic Regression\n",
    "    step 1) train\n",
    "    step 2) report\n",
    "    step 3) tune\n",
    "    step 4) report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "0.8910585817060637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l2', solver = 'saga')\n",
    "lr.fit(train_x_tf, train_y)\n",
    "\n",
    "# predict the label of the test samples here using the model\n",
    "test_predicted_lr = lr.predict(test_x_tf)\n",
    "\n",
    "# get model acuracy score\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy_score(test_y, test_predicted_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE - tune report\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      "[[2324   38]\n",
      " [ 280  277]]\n",
      "\n",
      "Macro F1-score: 0.79\n",
      "\n",
      "ROC AUC: 0.7406094704150241\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.94      2604\n",
      "           1       0.50      0.88      0.64       315\n",
      "\n",
      "    accuracy                           0.89      2919\n",
      "   macro avg       0.74      0.89      0.79      2919\n",
      "weighted avg       0.93      0.89      0.90      2919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_y, test_predicted_lr)\n",
    "print('Confusion Matrix: \\n')\n",
    "print(confusion)\n",
    "\n",
    "# f1\n",
    "print('\\nMacro F1-score: {:.2f}\\n'.format(f1_score(test_y, test_predicted_lr, average='macro')))\n",
    "\n",
    "# roc auc score\n",
    "fpr, tpr, t =  roc_curve(test_y, test_predicted_lr)\n",
    "roc_score = auc(fpr, tpr)\n",
    "print(\"ROC AUC:\", roc_score)\n",
    "\n",
    "# classification report\n",
    "print(\"\\nClassification report: \\n\")\n",
    "print(classification_report(test_predicted_lr, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertune\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/logantucker/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "240 fits failed out of a total of 480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/logantucker/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/logantucker/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/logantucker/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/logantucker/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.86318995        nan 0.86318995        nan 0.86318995\n",
      "        nan 0.86318995        nan 0.87274458        nan 0.87274458\n",
      "        nan 0.87274458        nan 0.87274458        nan 0.88583936\n",
      "        nan 0.88583936        nan 0.88583936        nan 0.88583936\n",
      "        nan 0.89177769        nan 0.89177769        nan 0.89177769\n",
      "        nan 0.89177769        nan 0.88679102        nan 0.88679102\n",
      "        nan 0.88679102        nan 0.88679102        nan 0.89250095\n",
      "        nan 0.89250095        nan 0.89250095        nan 0.89250095\n",
      "        nan 0.88629616        nan 0.88629616        nan 0.88629616\n",
      "        nan 0.88629616        nan 0.89227255        nan 0.89227255\n",
      "        nan 0.89227255        nan 0.89227255        nan 0.89265322\n",
      "        nan 0.89265322        nan 0.89265322        nan 0.89265322\n",
      "        nan 0.89539399        nan 0.89539399        nan 0.89539399\n",
      "        nan 0.89539399        nan 0.893681          nan 0.893681\n",
      "        nan 0.893681          nan 0.893681          nan 0.89638371\n",
      "        nan 0.89638371        nan 0.89638371        nan 0.89638371]\n",
      "  warnings.warn(\n",
      "/Users/logantucker/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan 0.86469357        nan 0.86469357        nan 0.86469357\n",
      "        nan 0.86469357        nan 0.87828321        nan 0.87828321\n",
      "        nan 0.87828321        nan 0.87828321        nan 0.93009136\n",
      "        nan 0.93009136        nan 0.93009136        nan 0.93009136\n",
      "        nan 0.94325276        nan 0.94325276        nan 0.94325276\n",
      "        nan 0.94325276        nan 0.94146365        nan 0.94146365\n",
      "        nan 0.94146365        nan 0.94146365        nan 0.95236962\n",
      "        nan 0.95236962        nan 0.95236962        nan 0.95236962\n",
      "        nan 0.93601066        nan 0.93601066        nan 0.93601066\n",
      "        nan 0.93601066        nan 0.94800152        nan 0.94800152\n",
      "        nan 0.94800152        nan 0.94800152        nan 0.99736391\n",
      "        nan 0.99736391        nan 0.99736391        nan 0.99736391\n",
      "        nan 0.99676437        nan 0.99676437        nan 0.99676437\n",
      "        nan 0.99676437        nan 0.99941949        nan 0.99941949\n",
      "        nan 0.99941949        nan 0.99941949        nan 0.99941949\n",
      "        nan 0.99941949        nan 0.99941949        nan 0.99941949]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: \n",
      "{'C': 10, 'fit_intercept': False, 'max_iter': 300, 'penalty': 'l2'}\n",
      "\n",
      "Best accuracy: \n",
      "0.8963837076513134\n"
     ]
    }
   ],
   "source": [
    "param_grid_lr = [{\n",
    "    \"penalty\": ['l1', 'l2'], \n",
    "    \"fit_intercept\": [True, False], \n",
    "    \"C\": [0.1, .9, 1.1, 1, 5, 10], \n",
    "    \"max_iter\": [300, 400, 500, 550]\n",
    "}]\n",
    "\n",
    "lr_gs = LogisticRegression()\n",
    "\n",
    "grid_search = GridSearchCV(lr_gs, param_grid_lr, cv=5, scoring=\"accuracy\", return_train_score=True)\n",
    "\n",
    "# train it\n",
    "grid_search.fit(train_x_tf, train_y)\n",
    "\n",
    "# get model with best params\n",
    "best_params_lr = grid_search.best_params_\n",
    "print(\"Best parameters: \")\n",
    "print(best_params_lr)\n",
    "\n",
    "# get best score\n",
    "best_score_lr = grid_search.best_score_\n",
    "print(\"\\nBest accuracy: \")\n",
    "print(best_score_lr)\n",
    "\n",
    "# get predected y of best model\n",
    "tuned_test_predicted_lr = grid_search.predict(test_x_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST - tune report\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      "[[2287   75]\n",
      " [ 220  337]]\n",
      "\n",
      "Macro F1-score: 0.82\n",
      "\n",
      "ROC AUC: 0.7866370890384408\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      2507\n",
      "           1       0.61      0.82      0.70       412\n",
      "\n",
      "    accuracy                           0.90      2919\n",
      "   macro avg       0.79      0.87      0.82      2919\n",
      "weighted avg       0.92      0.90      0.90      2919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_y, tuned_test_predicted_lr)\n",
    "print('Confusion Matrix: \\n')\n",
    "print(confusion)\n",
    "\n",
    "# f1\n",
    "print('\\nMacro F1-score: {:.2f}\\n'.format(f1_score(test_y, tuned_test_predicted_lr, average='macro')))\n",
    "\n",
    "# roc auc score\n",
    "fpr, tpr, t =  roc_curve(test_y, tuned_test_predicted_lr)\n",
    "roc_score = auc(fpr, tpr)\n",
    "print(\"ROC AUC:\", roc_score)\n",
    "\n",
    "# classification report\n",
    "print(\"\\nClassification report: \\n\")\n",
    "print(classification_report(tuned_test_predicted_lr, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier #3 : Support vector machines\n",
    "    step 1) train\n",
    "    step 2) report\n",
    "    step 3) tune\n",
    "    step 4) report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "0.8982528263103803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "sv = LinearSVC()\n",
    "sv.fit(train_x_tf, train_y)\n",
    "\n",
    "# predict the label of the test samples here using the model\n",
    "test_predicted_sv = sv.predict(test_x_tf)\n",
    "\n",
    "# get model acuracy score\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy_score(test_y, test_predicted_sv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE - tune report\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      "[[2291   71]\n",
      " [ 226  331]]\n",
      "\n",
      "Macro F1-score: 0.81\n",
      "\n",
      "ROC AUC: 0.7820978326799094\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      2517\n",
      "           1       0.59      0.82      0.69       402\n",
      "\n",
      "    accuracy                           0.90      2919\n",
      "   macro avg       0.78      0.87      0.81      2919\n",
      "weighted avg       0.92      0.90      0.90      2919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_y, test_predicted_sv)\n",
    "print('Confusion Matrix: \\n')\n",
    "print(confusion)\n",
    "\n",
    "# f1\n",
    "print('\\nMacro F1-score: {:.2f}\\n'.format(f1_score(test_y, test_predicted_sv, average='macro')))\n",
    "\n",
    "# roc auc score\n",
    "fpr, tpr, t =  roc_curve(test_y, test_predicted_sv)\n",
    "roc_score = auc(fpr, tpr)\n",
    "print(\"ROC AUC:\", roc_score)\n",
    "\n",
    "# classification report\n",
    "print(\"\\nClassification report: \\n\")\n",
    "print(classification_report(test_predicted_sv, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertune\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: \n",
      "{'C': 1, 'dual': True, 'loss': 'hinge', 'max_iter': 800, 'penalty': 'l2'}\n",
      "\n",
      "Best accuracy: \n",
      "0.8954701180053293\n"
     ]
    }
   ],
   "source": [
    "param_grid_sv = [{\n",
    "    \"penalty\": ['l2'], \n",
    "    \"loss\": ['hinge'], \n",
    "    \"dual\": [True], \n",
    "    \"C\": [0.5, 1, 1.2], \n",
    "    \"max_iter\": [800, 900]\n",
    "}]\n",
    "    \n",
    "sv_gs = LinearSVC()\n",
    "\n",
    "grid_search = GridSearchCV(sv_gs, param_grid_sv, cv=5, scoring=\"accuracy\", return_train_score=True)\n",
    "\n",
    "# train it\n",
    "grid_search.fit(train_x_tf, train_y)\n",
    "\n",
    "# get model with best params\n",
    "best_params_sv = grid_search.best_params_\n",
    "print(\"Best parameters: \")\n",
    "print(best_params_sv)\n",
    "\n",
    "# get best score\n",
    "best_score_sv = grid_search.best_score_\n",
    "print(\"\\nBest accuracy: \")\n",
    "print(best_score_sv)\n",
    "\n",
    "# get predected y of best model\n",
    "tuned_test_predicted_sv = grid_search.predict(test_x_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST - tune report\n",
    "***************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[2283   79]\n",
      " [ 210  347]]\n",
      "\n",
      "Macro F1-score: 0.82\n",
      "\n",
      "ROC AUC: 0.7947670096698627\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      2493\n",
      "           1       0.62      0.81      0.71       426\n",
      "\n",
      "    accuracy                           0.90      2919\n",
      "   macro avg       0.79      0.87      0.82      2919\n",
      "weighted avg       0.92      0.90      0.91      2919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_y, tuned_test_predicted_sv)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "# f1\n",
    "print('\\nMacro F1-score: {:.2f}\\n'.format(f1_score(test_y, tuned_test_predicted_sv, average='macro')))\n",
    "\n",
    "# roc auc score\n",
    "fpr, tpr, t =  roc_curve(test_y, tuned_test_predicted_sv)\n",
    "roc_score = auc(fpr, tpr)\n",
    "print(\"ROC AUC:\", roc_score)\n",
    "\n",
    "# classification report\n",
    "print(\"\\nClassification report: \\n\")\n",
    "print(classification_report(tuned_test_predicted_sv, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37d5838acf7412d52d2218a2c617f4d17188a79de14a5eefc87d0e41a4e00d29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
